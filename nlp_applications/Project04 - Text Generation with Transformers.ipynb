{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project08%20-%20Text%20Generation%20with%20Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLB25msp3Xen"
   },
   "source": [
    "# Text Generation with Transformers\n",
    "\n",
    "It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to mask future tokens – a valuable feature when it’s generating a translation word by word.\n",
    "\n",
    "Here we will use the GPT-2 Model to generate text based on an input sequence of text.\n",
    "\n",
    "![](https://i.imgur.com/z4k1IzU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5yq8CrH4ibV"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "9Ba8d9r3wbOI",
    "outputId": "4122a74f-7a95-48d6-9c49-beafc6b0bd13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/b5/2d78e74001af0152ee61d5ad4e290aec9a1e43925b21df2dc74ec100f1ab/pytorch_transformers-1.0.0-py3-none-any.whl (137kB)\n",
      "\r",
      "\u001b[K     |██▍                             | 10kB 20.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 20kB 6.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 30kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 40kB 5.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 51kB 7.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 61kB 8.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 71kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 81kB 10.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 92kB 11.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 102kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 112kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 122kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 133kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 143kB 9.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.4)\n",
      "Collecting sentencepiece (from pytorch-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 23.8MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 32.4MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30kB 38.5MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 40kB 41.5MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 51kB 44.0MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 61kB 47.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 71kB 50.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 81kB 51.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 92kB 53.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 102kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 112kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 122kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 133kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 143kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 153kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 163kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 174kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 184kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 194kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 204kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 215kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 225kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 235kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 245kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 256kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 266kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 276kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 286kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 296kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 307kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 317kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 327kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 337kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 348kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 358kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 368kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 378kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 389kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 399kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 409kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 419kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 430kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 440kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 450kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 460kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 471kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 481kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 491kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 501kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 512kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 522kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 532kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 542kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 552kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 563kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 573kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 583kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 593kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 604kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 614kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 624kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 634kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 645kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 655kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 665kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 675kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 686kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 696kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 706kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 716kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 727kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 737kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 747kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 757kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 768kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 778kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 788kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 798kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 808kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 819kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 829kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 839kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 849kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 860kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 870kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 880kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 890kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 901kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 911kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 921kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 931kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 942kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 952kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 962kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 972kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 983kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 993kB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 1.0MB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.0MB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.0MB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.0MB 55.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.0MB 55.5MB/s \n",
      "\u001b[?25hCollecting regex (from pytorch-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
      "\u001b[K     |████████████████████████████████| 655kB 63.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.199)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.199 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.199)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.199->boto3->pytorch-transformers) (2.5.3)\n",
      "Requirement already satisfied: docutils<0.15,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.199->boto3->pytorch-transformers) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.199->boto3->pytorch-transformers) (1.12.0)\n",
      "Building wheels for collected packages: regex\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for regex: filename=regex-2019.6.8-cp36-cp36m-linux_x86_64.whl size=604147 sha256=d699073966ed82702f676a7257d316709a31062de5634202b97d1c12c38f5507\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
      "Successfully built regex\n",
      "Installing collected packages: sentencepiece, regex, pytorch-transformers\n",
      "Successfully installed pytorch-transformers-1.0.0 regex-2019.6.8 sentencepiece-0.1.82\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvmCi8wt4k5G"
   },
   "source": [
    "# Load GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvU2C3WUwrJa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aME-hHZjwwNT",
    "outputId": "78223fa8-1692-4d8a-d31d-4dfe45379e3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1042301/1042301 [00:00<00:00, 2064188.98B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 1331602.62B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRYKn1dA4pBk"
   },
   "source": [
    "# Next Word Generation with GPT-2\n",
    "\n",
    "GPT-2 is a successor of GPT, the original NLP framework by OpenAI. The full GPT-2 model has 1.5 billion parameters, which is almost 10 times the parameters of GPT. GPT-2 give State-of-the Art results as you might have surmised already (and will soon see when we get into Python).\n",
    "\n",
    "The pre-trained model contains data from 8 million web pages collected from outbound links from Reddit. \n",
    "\n",
    "![](https://i.imgur.com/TbnGbjX.png)\n",
    "\n",
    "The architecture of GPT-2 is based on the very famous Transformers concept that was proposed by Google in their paper “Attention is all you need”. The Transformer provides a mechanism based on encoder-decoders to detect input-output dependencies.\n",
    "\n",
    "At each step, the model consumes the previously generated symbols as additional input when generating the next output.\n",
    "\n",
    "![](https://i.imgur.com/0XSSXBd.png)\n",
    "\n",
    "Modifications in GPT-2 include:\n",
    "\n",
    "- The model uses larger context and vocabulary size\n",
    "- After the final self-attention block, an additional normalization layer is added\n",
    "- Similar to a residual unit of type “building block”, layer normalization is moved to the input of each sub-block. It has batch normalization applied before weight layers, which is different from the original type “bottleneck”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7t3HtgUkw0pX",
    "outputId": "91fa611c-0ce4-4c86-eed8-1e1255c6523d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14618, 284, 262, 1280, 1366, 3783, 4495, 340, 318]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Welcome to the open data science conference it is\"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EINvRPSzxDxP",
    "outputId": "299e9a70-6c87-4b9c-ce1d-a6ce326c596c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14618,   284,   262,  1280,  1366,  3783,  4495,   340,   318]])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokens_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eK3vlMWmxJpH",
    "outputId": "483782eb-9c42-4341-d3e9-ba0c2c4cd9c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:00<00:00, 43316.37B/s]\n",
      "100%|██████████| 548118077/548118077 [00:18<00:00, 29635831.08B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): BertLayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Y5w_1JK1xOy2",
    "outputId": "d504cbd8-144f-4bbc-b577-e9ce361fd1e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): BertLayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1)\n",
       "          (resid_dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ln_2): BertLayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): BertLayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5W2842GKxZ23"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1BHGaAVLxffT",
    "outputId": "9e01569d-f83d-47ad-eead-69e91c575135"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 50257])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yxdytwGIxhxp",
    "outputId": "8b55c8a2-7aa8-42a5-81b1-4373d587e9b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to the open data science conference it is a'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZwNnlQmx45H"
   },
   "outputs": [],
   "source": [
    "start = 'Natural Language Processing is slowly becoming'\n",
    "indexed_tokens = tokenizer.encode(start)\n",
    "\n",
    "for i in range(75):\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  tokens_tensor = tokens_tensor.to('cuda')\n",
    "  with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "    predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "    indexed_tokens = indexed_tokens + [predicted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "oFhvg-ouykk1",
    "outputId": "5028fa80-e5a1-401d-a34d-8bf54783ad1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is slowly becoming a reality.\n",
      "\n",
      "The first step is to create a language processing system that can be used to create a language. This is done by using a language processing system that is built on top of a language processing system.\n",
      "\n",
      "The language processing system is a set of tools that can be used to create a language. The language processing system is a set of tools that can can\n"
     ]
    }
   ],
   "source": [
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cB6wAfCu5_-q"
   },
   "source": [
    "# Paragraph Generation with GPT-2\n",
    "\n",
    "Refer to this [source code](https://github.com/huggingface/pytorch-transformers/blob/master/examples/run_generation.py#L106-L129) to deep dive.\n",
    "\n",
    "- `length`: It represents the number of tokens in the generated text. If the length is None, then the number of tokens is decided by model hyperparameters\n",
    "- `temperature`: This controls randomness in Boltzmann distribution. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions\n",
    "- `top_k`: This parameter controls diversity. If the value of top_k is set to 1, this means that only 1 word is considered for each step (token). If top_k is set to 40, that means 40 words are considered at each step. 0 (default) is a special setting meaning no restrictions. top_k = 40 generally is a good value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "r8mOAD-izApl",
    "outputId": "1ed73585-eb56-44b6-9eee-5a2d8e120578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-transformers'...\n",
      "remote: Enumerating objects: 5844, done.\u001b[K\n",
      "remote: Total 5844 (delta 0), reused 0 (delta 0), pack-reused 5844\u001b[K\n",
      "Receiving objects: 100% (5844/5844), 3.16 MiB | 5.45 MiB/s, done.\n",
      "Resolving deltas: 100% (4167/4167), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/pytorch-transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "teGOary50sLX",
    "outputId": "62d377d1-82a3-4144-ade6-5031158fe0d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/06/2019 07:29:38 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "08/06/2019 07:29:38 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "08/06/2019 07:29:39 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "08/06/2019 07:29:39 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"token_ids\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "08/06/2019 07:29:39 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "Namespace(device=device(type='cuda'), length=500, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, padding_text='', prompt='', seed=42, temperature=1.0, top_k=0, top_p=0.9)\n",
      "Model prompt >>> Natural language processing is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human languages\n",
      "100% 500/500 [00:16<00:00, 29.66it/s]\n",
      ". Computers have learned to sense and interpret abstractions and data as they interact with information and some other functional and computational categories. Through formal induction, they learn to understand human language and associate relationships with different abstractions and data.\n",
      "\n",
      "Besides the continuous learning of various categories and entities, semantic linguists have developed powerful methods to describe human language and information exchange. These methods support three main goals: (1) to develop theory about human language and knowledge of language and with it human language processing, (2) to learn systems of probabilistic and semantic data processing to increase the human language perception and language-processing, and (3) to participate in conversations about human language and human languages on occasion with AI and other entities such as AI can enhance (Learn AI @ YouTube, May 2015; discover it here.)\n",
      "\n",
      "The main benefit of classification is the positive development of understanding and understanding language and, in particular, the better understanding and understanding how to correctly distinguish the identity of words and phrases. As a result, a normal human speech comprehension requires reading and processing many dozens of different formats in an hour or more. A strong understanding of human language, a strong understanding of words, and a good understanding of what language, on a macro level, actually means will help us obtain a greater understanding of what sounds, understand. It is this important understanding which enables software to learn the essential features of the human language.\n",
      "\n",
      "The principal virtue of human language processing is its recognition of quantitative information (y and z, Z and E) but it does not directly represent biological information about words. Where human language processing involves words and not biological information, natural language processing offers a theoretical description of mathematical functions as logarithmically oriented variable functions, whereas AI recognizes formal algebraic interactions between words and data and performs symbolic symbolic operations. While these two principles of human language processing may help us infer quantum information from mathematical concepts in general, they don't imply that we have directly come to know and understand words and phrases at their natural rhythms. It is at this point in time that robot processing began.\n",
      "\n",
      "The purpose of the Howbot module is to illustrate how robots construct or model concepts through logical abstraction and not inferences from a raw data set. For example, to build a robot \"without knowing\" this data set is a far more interesting thing than typing a particular sentence, that data is only intended to be processed by robots. However, if we present the application of the data as a\n",
      "Model prompt >>> Natural Language Processing is slowly becoming a reality.\n",
      "100% 500/500 [00:16<00:00, 30.68it/s]\n",
      "\n",
      "\n",
      "Despite a lot of 'big' programming tools, with'moving parts' being a feature, the best approaches to integrating in programming languages are those that only need to be coupled with relatively small categories of other programming features.\n",
      "\n",
      "Smaller groups\n",
      "\n",
      "Languages with functional languages like C are key to the way technology evolves as we all increase our understanding of how something works.\n",
      "\n",
      "We need to make choices that allow us to continuously implement these small concepts, resulting in language building in the fields that are under our control.\n",
      "\n",
      "There are a lot of stakeholders that are first and foremost interested in common themes in software development and all major platforms. While they should be able to quickly and logically fix conflicts and fix deficiencies in each platform, we need to actively engage with those stakeholders.\n",
      "\n",
      "Increasing engagement is important when using a language's specific structures and features, which adds to their development agility. When we need to take common approach to tackling issues other languages have generated, we should prioritize fewer non-talk languages.\n",
      "\n",
      "Developing language barrier\n",
      "\n",
      "If language barrier is a problem, then we need to start building change flow around it and generate a specific language barrier.\n",
      "\n",
      "These barriers are something that needs to be tackled if we want to take the programming world more forward. First, by tackling the language barrier we simply show that languages that, by and large, need to remain internationalized are not coming with any measurable short-term performance benefits, and improve at an accelerated rate that is not reflected in their average results.\n",
      "\n",
      "This is a problem for languages. The C++ team on that team stated that any language having any longer than 2-3 lines of C++ or C# core, can't compile.\n",
      "\n",
      "Many of us fail to see the importance of translating a lot of code into C++ to support mainstream use cases. This can make it more difficult to integrate functional languages with new native features and it is a prime example of a Lisp problem where the project becomes huge. As the language allows for multithreading and other purely syntactic reasons that keep the typical LLVM compiler on the engine, we are going to fail to implement well-behaved C++ and are going to need to refactor the language to a more functional focus.\n",
      "\n",
      "Programming in general also needs to integrate a lot of languages in their design patterns that it is difficult to maintain without tooling these cultures with new languages. To that end we need to\n",
      "Model prompt >>> The flying cats seem to have taken over the world.\n",
      "100% 500/500 [00:16<00:00, 30.80it/s]\n",
      " They moved from world to world, from tiny Mauna Loa nesting in their pill box to tiny kitchens on their dusty walls. They made it through the finch, into grandma's life.\n",
      "\n",
      "In 1985, in Malabar, Indonesia, for a donation of $2,500, Serena was the last resort for the cats. She barely knew where they lived, because it was so sad to lose her beloved home. But she made a serious one-night stand with the family of Serena, who is still a skilled cat handler. Today, even for very little money, she knows all about cat rescues. \"It is a gift for me, a chance to help spread the word about cats,\" she says. \"I'm very proud of the guys who came together to help a pet in need.\"\n",
      "\n",
      "Showing gratitude to Serena as a part of all this, one of the Serena Cats board of trustees has called in the University of Arizona to save her and place her at the center of her family. The same day, the nonprofit Animal Society of Arizona held an event on her behalf, in the little Golden Gate Field to make animals happy. And for Serena, every gesture at this event felt really good.\n",
      "\n",
      "\"All these things you need to work on to get animals happy,\" Serena says. \"There's just a profound sense that the animals don't want to be upset by these things. That's so easy to make. It was great to finally be there for them. We didn't come to help them cause their problems. So the more we helped them in the beginning, the more they said, 'Gosh, we're doing this for them,' \" Serena says. \"People all want to help animals in need; people have broken their hearts so many times to meet them. I don't think it's anything I can go through right now to try and do it right. Hopefully in the next couple years we'll help end this culture of grandiose attachment.\"\n",
      "\n",
      "Sign up for weekly Fit for Cat email updates from our scientists, nurses and educators.\n",
      "\n",
      "Fork this: $1 of free support donations | View food donation forms from our people\n",
      "\n",
      "Tweet this: $1 of free clothing donations\n",
      "\n",
      "The protein burrito is only half of what it used to be. Then, from 1985 until 2009, in Amador, just outside of Las Cruces, Javier Ozanne worked\n",
      "Model prompt >>> Demonetization was a sudden shocker in India!\n",
      "100% 500/500 [00:16<00:00, 30.77it/s]\n",
      "\n",
      "\n",
      "Indians go to a lot of temples for anti-Asian ideas. It's very painful. So is holding Supreme Court since, if someone puts a wall down, you see people come back. Other holy sites all over India are being set up. So they put up shrines, churches, statues. It's becoming very difficult to keep up with them all. There is definitely political turmoil. As a country, we are living in a bubble of blood and drugs. So the government is not worried about the people, but it is definitely worried about the Kashmiri population and Kashmiris. I hope there is a change and to give Kashmiri political leadership some voice. This is a democratic society, I believe.\n",
      "\n",
      "What are your thoughts on the debate around territorial concessions?\n",
      "\n",
      "I don't really see a dispute to give Kashmiris an option. I would like them to consider it as a political issue. As it stands, the states don't have a right to stop tribal autonomy. It is a sovereign project of India. Do we want to stay or do we want to set a precedent? This is not the first time that things were problematic, so is this the second time that people are angry at the political leadership for making land concessions for a political project? Is there anything wrong with a political project to return to India?\n",
      "\n",
      "Here is what I would like to do. I would like to talk about Kashmir in the framework of the larger international context of resolving the struggle in Kashmir. From the international perspective, it is very important to talk about the question of Kashmir in the context of India. The bigger picture, in my view, is that all of India can follow the decision of the international community on whether to take the right action or not in this regard.\n",
      "\n",
      "You must show some willingness to think to advance any kind of political and international principle. After all, what matters more is whether India has prepared (or should have prepared) this short-term solution to its problematic situation in the north-eastern states of Jammu and Kashmir, the most serious part of the central Kashmir issue that divides Kashmiris from the bulk of the people in the east.\n",
      "\n",
      "India has not looked at where it has been doing all this. As the international situation changes, it will get clearer. In our experience of Kashmir, the relief efforts have shown that if these efforts are successful, they are able to bring about reform. However, at the same time the\n",
      "Model prompt >>> Finally we saw Gandalf, Frodo and Harry Potter go ahead and\n",
      "100% 500/500 [00:15<00:00, 17.87it/s]\n",
      " kill him with Mace, the sorcerer was considered a poor man and only could take it, as if Sauron could destroy the world through spells. But while (Dumbledore being an angel) Gandalf is willing to kill many who they thought were evil, Gandalf is incapable of choosing between harming, running away and healing (including of himself, of course). This always happens.\n",
      "\n",
      "Willing to'stay as its victim' is one of the most central aspects of a story.\n",
      "\n",
      "The phrase \"But of course you'll stay as its victim\" is also quite a classic description. We see it used in casting out a terror, defending ourselves, and attacking the villain (literally or figuratively, only if we can at least take out a parent). When we see \"But of course\" is not an insult we accept it as true, it is an expectation we assume we're not accountable. We would never do that in our own lives.\n",
      "\n",
      "When faced with a power, that that power is the same to all of us, to all of existence.\n",
      "\n",
      "Sure, having a powerful magical force keep us safe, makes us stronger, makes us great, but only to some extent (since there is a power involved). Although it is true that violence and destruction are given no due consideration in the story of Lúcio, it is always the greatest temptation, and the only way to have that power at least makes for some wonderful magical source to work (the ones we had been hoping for would make wonderful stories). As well, you can't win battles for a burning desire to ensure that you'll be sent back to the Abyss (don't believe me? See there are thousands and thousands of reasons that it's not possible, or even possible that there are more than you could ever hope to understand!). The true essence of Lúcio is the personal respect and responsibility that comes from giving up one's hope in our own chosen destiny (who we are). The memories that we can lose are not more precious than our lives (we don't realize, only that they will be). Those choices you've made are actually your ability to advance, your power rather than control, to bring about your satisfaction.\n",
      "\n",
      "As much as Gandalf and Frodo think it's okay to kill us and take us to the Abyss, even the very same can happen with the good and the evil.\n",
      "\n",
      "This leaves an infinite number of possibilities: Gandalf is\n",
      "Model prompt >>> \n",
      "  0% 0/500 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"pytorch-transformers/examples/run_generation.py\", line 195, in <module>\n",
      "    main()\n",
      "  File \"pytorch-transformers/examples/run_generation.py\", line 184, in main\n",
      "    is_xlnet=bool(args.model_type == \"xlnet\"),\n",
      "  File \"pytorch-transformers/examples/run_generation.py\", line 124, in sample_sequence\n",
      "    outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_transformers/modeling_gpt2.py\", line 595, in forward\n",
      "    past=past, head_mask=head_mask)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_transformers/modeling_gpt2.py\", line 493, in forward\n",
      "    input_ids = input_ids.view(-1, input_ids.size(-1))\n",
      "RuntimeError: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n"
     ]
    }
   ],
   "source": [
    "!python pytorch-transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=500 \\\n",
    "    --model_name_or_path=gpt2 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFK78k0J0wLz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Generation with Transformers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
